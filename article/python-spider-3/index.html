<!doctype html><html lang=zh><head><title>Python爬虫从入门到放弃（三）Urllib库的基本使用 - ShiuxのBlog</title><meta charset=UTF-8><meta name=keywords content=""><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5"><link rel="shortcut icon" href=/logo.svg type=image/x-icon><meta name=description content="什么是Urllib Urllib是python内置的HTTP请求库，包括以下模块  urllib.request - 请求模块 urllib.error - 异常处理模块 urllib.parse - url解析模块 urllib.robotparser - robots.txt解析模块  Urllib基本使用 urlopen 关于urllib.request.urlopen参数的介绍： 1url"><meta property=og:type content=article><meta property=og:title content=Python爬虫从入门到放弃（三）Urllib库的基本使用><meta property=og:url content=https://blog.shiux.com/article/python-spider-3/index.html><meta property=og:site_name content=ShiuxのBlog><meta property=og:description content="什么是Urllib Urllib是python内置的HTTP请求库，包括以下模块  urllib.request - 请求模块 urllib.error - 异常处理模块 urllib.parse - url解析模块 urllib.robotparser - robots.txt解析模块  Urllib基本使用 urlopen 关于urllib.request.urlopen参数的介绍： 1url"><meta property=og:locale content=zh_CN><meta property=og:image content=https://blog.shiux.com/images/9403f483f0a7742c99e2f00d3e87b379.png><meta property=og:image content=https://blog.shiux.com/images/1087ab61706e023b577651442ada2c4a.png><meta property=og:image content=https://blog.shiux.com/images/fefb814a45d52816fab98dc81d672b47.png><meta property=og:image content=https://blog.shiux.com/images/18bf4f402d6c09219fc6e574a3f195f8.png><meta property=og:image content=https://blog.shiux.com/images/71eb3fb011c290339e9ba1f1db60ea77.png><meta property=og:image content=https://blog.shiux.com/images/2ea662510c6b6b6119dcffa5038ed44a.png><meta property=og:image content=https://blog.shiux.com/images/53d7c7f3daf4e17112e1806425372266.png><meta property=article:published_time content=2021-09-04T15:33:05.000Z><meta property=article:modified_time content=2024-10-17T00:08:30.063Z><meta property=article:author content=Shiux><meta property=article:tag content=Spider><meta name=twitter:card content=summary><meta name=twitter:image content=https://blog.shiux.com/images/9403f483f0a7742c99e2f00d3e87b379.png><link rel=stylesheet href=/lib/fancybox/fancybox.css><link rel=stylesheet href=/lib/mdui_043tiny/mdui.css><link rel=stylesheet href="/lib/iconfont/iconfont.css?v=1729123720306"><link rel=stylesheet href="/css/style.css?v=1729123720306"><link rel=stylesheet href="/custom.css?v=1729123720306"><script src=/lib/mdui_043tiny/mdui.js async></script><script src=/lib/fancybox/fancybox.umd.js async></script><script src=/lib/lax.min.js async></script><script async src="/js/app.js?v=1729123720306"></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/atom-one-dark.min.css><meta name=generator content="Hexo 7.3.0"></head><body class="nexmoe mdui-drawer-body-left"><div id=nexmoe-background><div class=nexmoe-bg style=background-image:url(https://www.dmoe.cc/random.php)></div><div class="mdui-appbar mdui-shadow-0"><div class=mdui-toolbar><a class="mdui-btn mdui-btn-icon mdui-ripple" mdui-drawer="{target: &#039;#drawer&#039;, swipe: true}" title=menu><i class="mdui-icon nexmoefont icon-menu"></i></a><div class=mdui-toolbar-spacer></div><a class="mdui-btn mdui-btn-icon" href=/ title=Shiux><img src=/logo.svg alt=Shiux></a></div></div></div><div id=nexmoe-header><div class="nexmoe-drawer mdui-drawer" id=drawer><div class="nexmoe-avatar mdui-ripple"><a href=/ title=Shiux><img src=/logo.svg alt=Shiux alt=Shiux></a></div><div class=nexmoe-count><div><span>文章</span>50</div><div><span>标签</span>36</div><div><span>分类</span>7</div></div><div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}"><a class="nexmoe-list-item mdui-list-item mdui-ripple false" href=/ title=回到首页><i class="mdui-list-item-icon nexmoefont icon-home"></i><div class=mdui-list-item-content>回到首页</div></a><a class="nexmoe-list-item mdui-list-item mdui-ripple false" href=/archive.html title=文章归档><i class="mdui-list-item-icon nexmoefont icon-container"></i><div class=mdui-list-item-content>文章归档</div></a><a class="nexmoe-list-item mdui-list-item mdui-ripple false" href=/about.html title=关于博主><i class="mdui-list-item-icon nexmoefont icon-info-circle"></i><div class=mdui-list-item-content>关于博主</div></a></div><div class=nexmoe-widget-wrap><div class="nexmoe-widget nexmoe-social"><a class=mdui-ripple href=https://github.com/shiucs/ target=_blank mdui-tooltip="{content: 'GitHub'}" style=color:#191717;background-color:rgba(25,23,23,.1)><i class="nexmoefont icon-github"></i> </a><a class=mdui-ripple href=https://www.zhihu.com/people/fungyua target=_blank mdui-tooltip="{content: '知乎'}" style=color:#1e88e5;background-color:rgba(30,136,229,.1)><i class="nexmoefont icon-zhihu"></i> </a><a class=mdui-ripple href=/atom.xml target=_blank mdui-tooltip="{content: 'RSS'}" style=color:#f78422;background-color:rgba(247,132,34,.1)><i class="nexmoefont icon-rss"></i></a></div></div><div class=nexmoe-widget-wrap><h3 class=nexmoe-widget-title>文章分类</h3><div class=nexmoe-widget><ul class=category-list><li class=category-list-item><a class=category-list-link href=/categories/Code/ >Code</a> <span class=category-list-count>20</span></li><li class=category-list-item><a class=category-list-link href=/categories/Guide/ >Guide</a> <span class=category-list-count>11</span></li><li class=category-list-item><a class=category-list-link href=/categories/Linux/ >Linux</a> <span class=category-list-count>5</span></li><li class=category-list-item><a class=category-list-link href=/categories/MacOS/ >MacOS</a> <span class=category-list-count>5</span></li><li class=category-list-item><a class=category-list-link href=/categories/NetWork/ >NetWork</a> <span class=category-list-count>2</span></li><li class=category-list-item><a class=category-list-link href=/categories/UX/ >UX</a> <span class=category-list-count>1</span></li><li class=category-list-item><a class=category-list-link href=/categories/Windows/ >Windows</a> <span class=category-list-count>5</span></li></ul></div></div><div class=nexmoe-widget-wrap><div id=randomtagcloud class="nexmoe-widget tagcloud nexmoe-rainbow"><a href=/tags/APP/ style=font-size:13.33px>APP</a> <a href=/tags/AdGuard/ style=font-size:11.67px>AdGuard</a> <a href=/tags/Aria2/ style=font-size:10px>Aria2</a> <a href=/tags/CentOS/ style=font-size:13.33px>CentOS</a> <a href=/tags/Cloud/ style=font-size:10px>Cloud</a> <a href=/tags/DNS/ style=font-size:10px>DNS</a> <a href=/tags/Docker/ style=font-size:10px>Docker</a> <a href=/tags/FastAPI/ style=font-size:10px>FastAPI</a> <a href=/tags/Git/ style=font-size:11.67px>Git</a> <a href=/tags/HTML/ style=font-size:11.67px>HTML</a> <a href=/tags/Hackintosh/ style=font-size:11.67px>Hackintosh</a> <a href=/tags/Hexo/ style=font-size:11.67px>Hexo</a> <a href=/tags/IP/ style=font-size:11.67px>IP</a> <a href=/tags/Issue/ style=font-size:11.67px>Issue</a> <a href=/tags/JavaScript/ style=font-size:15px>JavaScript</a> <a href=/tags/JetBrains/ style=font-size:10px>JetBrains</a> <a href=/tags/MacOS/ style=font-size:10px>MacOS</a> <a href=/tags/MySQL/ style=font-size:11.67px>MySQL</a> <a href=/tags/NGINX/ style=font-size:10px>NGINX</a> <a href=/tags/NextCloud/ style=font-size:11.67px>NextCloud</a> <a href=/tags/Nodejs/ style=font-size:15px>Nodejs</a> <a href=/tags/PHP/ style=font-size:13.33px>PHP</a> <a href=/tags/PIP/ style=font-size:10px>PIP</a> <a href=/tags/Powershell/ style=font-size:10px>Powershell</a> <a href=/tags/SQLAlchemy/ style=font-size:10px>SQLAlchemy</a> <a href=/tags/SSH/ style=font-size:10px>SSH</a> <a href=/tags/Spider/ style=font-size:18.33px>Spider</a> <a href=/tags/Ubuntu/ style=font-size:11.67px>Ubuntu</a> <a href=/tags/npm/ style=font-size:11.67px>npm</a> <a href=/tags/Code/ style=font-size:13.33px>代码</a> <a href=/tags/FrontEnd/ style=font-size:11.67px>前端</a> <a href=/tags/Consult/ style=font-size:16.67px>咨询</a> <a href=/tags/Skill/ style=font-size:20px>技巧</a> <a href=/tags/Note/ style=font-size:20px>笔记</a> <a href=/tags/Terminal/ style=font-size:15px>终端</a> <a href=/tags/Sources/ style=font-size:10px>软件源</a></div><script>for(var maxTagcloud=parseInt(17),tags_length=parseInt(36),tags_arr=[],i=0;i<tags_length;i++)tags_arr.push(i);tags_arr.sort(function(a,t){return.5<Math.random()?-1:1});for(var tags_arr=tags_arr.slice(0,maxTagcloud<tags_length?tags_length-maxTagcloud:0),tag_i=0;tag_i<tags_arr.length;tag_i++)document.getElementById("randomtagcloud").children[tags_arr[tag_i]].style.display="none"</script></div><div class=nexmoe-widget-wrap><h3 class=nexmoe-widget-title>一言</h3><div class=nexmoe-widget><ul class=hitokoto-box><li id=hitokoto_text_parent class=hitokoto-text hitokotocategory=""><a href=# id=hitokoto_text></a> <a href=# id=hitokoto_error_text style=display:none></a></li></ul></div></div><script>let hitokotoText=document.getElementById("hitokoto_text"),hitokotoErroText=document.getElementById("hitokoto_error_text"),hitokotoCategory=document.getElementById("hitokoto_text_parent").getAttribute("hitokotoCategory");window.addEventListener("load",function(){let t="https://v1.hitokoto.cn";hitokotoCategory&&(t+="?c="+hitokotoCategory),fetch(t).then(t=>t.json()).then(t=>{hitokotoText.innerText="「 "+t.hitokoto+" 」 from "+t.from,hitokotoText.href="https://hitokoto.cn/?uuid="+t.uuid}).catch(t=>{console.error(11,t),hitokotoText.style.display="none",hitokotoErroText.style.display="block"})})</script><div class=nexmoe-widget-wrap><h3 class=nexmoe-widget-title>文章归档</h3><div class=nexmoe-widget><ul class=archive-list><li class=archive-list-item><a class=archive-list-link href=/archives/2024/ >2024</a><span class=archive-list-count>1</span></li><li class=archive-list-item><a class=archive-list-link href=/archives/2023/ >2023</a><span class=archive-list-count>9</span></li><li class=archive-list-item><a class=archive-list-link href=/archives/2022/ >2022</a><span class=archive-list-count>2</span></li><li class=archive-list-item><a class=archive-list-link href=/archives/2021/ >2021</a><span class=archive-list-count>17</span></li><li class=archive-list-item><a class=archive-list-link href=/archives/2020/ >2020</a><span class=archive-list-count>20</span></li><li class=archive-list-item><a class=archive-list-link href=/archives/2019/ >2019</a><span class=archive-list-count>1</span></li></ul></div></div><div class=nexmoe-widget-wrap><h3 class=nexmoe-widget-title>最新文章</h3><div class=nexmoe-widget><ul><li><a href=/article/parameters-for-installing-android-app/ >正确选择安装包，arm64-v8a、armeabi-v7a、x86等参数有何区别？</a></li><li><a href=/article/windows-port-is-occupied/ >解决Windows电脑端口被占用问题</a></li><li><a href=/article/python-pip-issue/ >修改了Python安装目录的文件夹名称而无法使用pip的两种实用解决方法</a></li><li><a href=/article/python-fastapi-sqlalchemy/ >Python FastAPI框架配合SQLAlchemy操作Mysql数据库 增删改查</a></li><li><a href=/article/powershell-perfect-setup/ >Powershell完美配置</a></li></ul></div></div><div class=nexmoe-copyright>&copy; 2024 Shiux Powered by <a href=http://hexo.io/ target=_blank>Hexo</a> & <a href=https://github.com/theme-nexmoe/hexo-theme-nexmoe target=_blank>Nexmoe</a></div></div></div><div id=nexmoe-content><div class=nexmoe-primary><div class=nexmoe-post><article><div class=nexmoe-post-cover><img src=https://www.dmoe.cc/random.php alt=Python爬虫从入门到放弃（三）Urllib库的基本使用 loading=lazy><h1>Python爬虫从入门到放弃（三）Urllib库的基本使用</h1></div><div class=nexmoe-post-meta><div class=nexmoe-rainbow><a class="nexmoefont icon-calendar-fill">2021年09月04日</a> <a class="nexmoefont icon-appstore-fill -link" href=/categories/Code/ >Code</a> <a><i class="nexmoefont icon-areachart"></i>约2.3k字</a> <a><i class="nexmoefont icon-time-circle-fill"></i>预计需要10分钟</a></div></div><h2 id=什么是Urllib>什么是Urllib</h2><p>Urllib是python内置的HTTP请求库，包括以下模块</p><ul><li><code>urllib.request</code> - 请求模块</li><li><code>urllib.error</code> - 异常处理模块</li><li><code>urllib.parse</code> - url解析模块</li><li><code>urllib.robotparser</code> - robots.txt解析模块</li></ul><h2 id=Urllib基本使用>Urllib基本使用</h2><h3 id=urlopen>urlopen</h3><p>关于<code>urllib.request.urlopen</code>参数的介绍：</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre></td><td class=code><pre><span class=line>urllib.request.urlopen(url, data=<span class=literal>None</span>, [timeout, ]*, cafile=<span class=literal>None</span>, capath=<span class=literal>None</span>, cadefault=<span class=literal>False</span>, context=<span class=literal>None</span>)</span><br></pre></td></tr></table></figure><ul><li>url：url 地址。</li><li>data：发送到服务器的其他数据对象，默认为 None。</li><li>timeout：设置访问超时时间。</li><li>cafile 和 capath：cafile 为 CA 证书， capath 为 CA 证书的路径，使用 HTTPS 需要用到。</li><li>cadefault：已经被弃用。</li><li>context：ssl.SSLContext类型，用来指定 SSL 设置。</li></ul><h4 id=url参数的使用>url参数的使用</h4><p>先写一个简单的例子：</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>import</span> urllib.request</span><br><span class=line></span><br><span class=line>response = urllib.request.urlopen(<span class=string>&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class=line><span class=built_in>print</span>(response.read().decode(<span class=string>&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure><p><code>response.read()</code>可以获取到网页的内容，如果没有<code>read()</code>，将返回如下内容</p><p><code>&lt;http.client.HTTPResponse object at 0x00000215D37663A0&gt;</code></p><h4 id=data参数的使用>data参数的使用</h4><p>上述的例子是通过get请求获得请求内容，下面使用urllib的post请求</p><p>这里通过<a target=_blank rel=noopener href=http://httpbin.org/post>http://httpbin.org/post</a>演示（该网站可以作为练习使用urllib的一个站点使用，可以<br>模拟各种请求操作）。</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>import</span> urllib.parse</span><br><span class=line><span class=keyword>import</span> urllib.request</span><br><span class=line></span><br><span class=line>data = <span class=built_in>bytes</span>(urllib.parse.urlencode(&#123;<span class=string>&#x27;word&#x27;</span>: <span class=string>&#x27;hello&#x27;</span>&#125;), encoding=<span class=string>&#x27;utf8&#x27;</span>)</span><br><span class=line><span class=built_in>print</span>(data)</span><br><span class=line>response = urllib.request.urlopen(<span class=string>&#x27;http://httpbin.org/post&#x27;</span>, data=data)</span><br><span class=line><span class=built_in>print</span>(response.read())</span><br></pre></td></tr></table></figure><p>这里就用到<code>urllib.parse</code>，通过<code>bytes(urllib.parse.urlencode())</code>可以将post数据进行转换放到<code>urllib.request.urlopen</code>的<code>data</code>参数中。这样就完成了一次post请求。</p><p>所以如果我们添加<code>data</code>参数的时候就是以post请求方式请求，如果没有data参数就是get请求方式</p><h4 id=timeout参数的使用>timeout参数的使用</h4><p>在某些网络情况不好或者服务器端异常的情况会出现请求慢的情况，或者请求异常，所以这个时候我们需要给请求设置一个超时时间，而不是让程序一直在等待结果。例子如下：</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>import</span> urllib.request</span><br><span class=line></span><br><span class=line>response = urllib.request.urlopen(<span class=string>&#x27;http://httpbin.org/get&#x27;</span>, timeout=<span class=number>1</span>)</span><br><span class=line><span class=built_in>print</span>(response.read())</span><br></pre></td></tr></table></figure><p>运行之后我们看到可以正常的返回结果，接着我们将timeout时间设置为0.1，运行程序会提示如下错误：</p><p><img onerror=imgOnError(this) data-fancybox=gallery src=/images/9403f483f0a7742c99e2f00d3e87b379.png alt="" data-caption="" loading=lazy></p><p>所以我们需要对异常进行抓取，代码更改为</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>import</span> socket</span><br><span class=line><span class=keyword>import</span> urllib.request</span><br><span class=line><span class=keyword>import</span> urllib.error</span><br><span class=line></span><br><span class=line><span class=keyword>try</span>:</span><br><span class=line>    response = urllib.request.urlopen(<span class=string>&#x27;http://httpbin.org/get&#x27;</span>, timeout=<span class=number>0.1</span>)</span><br><span class=line><span class=keyword>except</span> urllib.error.URLError <span class=keyword>as</span> e:</span><br><span class=line>    <span class=keyword>if</span> <span class=built_in>isinstance</span>(e.reason, socket.timeout):</span><br><span class=line>        <span class=built_in>print</span>(<span class=string>&#x27;TIME OUT&#x27;</span>)</span><br></pre></td></tr></table></figure><p>这样超时时返回以下内容：</p><p><img onerror=imgOnError(this) data-fancybox=gallery src=/images/1087ab61706e023b577651442ada2c4a.png alt="" data-caption="" loading=lazy></p><h4 id=响应>响应</h4><h5 id=响应类型、状态码、响应头>响应类型、状态码、响应头</h5><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>import</span> urllib.request</span><br><span class=line></span><br><span class=line>response = urllib.request.urlopen(<span class=string>&#x27;https://www.python.org&#x27;</span>)</span><br><span class=line><span class=built_in>print</span>(<span class=built_in>type</span>(response))</span><br></pre></td></tr></table></figure><p>可以看到结果为：</p><p><img onerror=imgOnError(this) data-fancybox=gallery src=/images/fefb814a45d52816fab98dc81d672b47.png alt="" data-caption="" loading=lazy></p><p>我们可以通过<code>response.status</code>、<code>response.getheaders()</code>，获取状态码以及头部信息</p><p><code>response.read()</code>获得的是响应体的内容</p><p>当然上述的<code>urlopen</code>只能用于一些简单的请求，因为它无法添加一些header信息，如果后面写爬虫我们可以知道，很多情况下我们是需要添加头部信息去访问目标站的，这个时候就用到了urllib.request</p><h3 id=request>request</h3><h4 id=设置Headers>设置Headers</h4><p>有很多网站为了防止程序爬虫爬网站造成网站瘫痪，会需要携带一些headers头部信息才能访问，最长见的有<code>user-agent</code>参数</p><p>写一个简单的例子：</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>import</span> urllib.request</span><br><span class=line></span><br><span class=line>request = urllib.request.Request(<span class=string>&#x27;https://python.org&#x27;</span>)</span><br><span class=line>response = urllib.request.urlopen(request)</span><br><span class=line><span class=built_in>print</span>(response.read().decode(<span class=string>&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure><p>给请求添加头部信息，从而定制自己请求网站是时的头部信息</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>from</span> urllib <span class=keyword>import</span> request, parse</span><br><span class=line></span><br><span class=line>url = <span class=string>&#x27;http://httpbin.org/post&#x27;</span></span><br><span class=line>headers = &#123;</span><br><span class=line>    <span class=string>&#x27;User-Agent&#x27;</span>: <span class=string>&#x27;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&#x27;</span>,</span><br><span class=line>    <span class=string>&#x27;Host&#x27;</span>: <span class=string>&#x27;httpbin.org&#x27;</span></span><br><span class=line>&#125;</span><br><span class=line><span class=built_in>dict</span> = &#123;</span><br><span class=line>    <span class=string>&#x27;name&#x27;</span>: <span class=string>&#x27;zhaofan&#x27;</span></span><br><span class=line>&#125;</span><br><span class=line>data = <span class=built_in>bytes</span>(parse.urlencode(<span class=built_in>dict</span>), encoding=<span class=string>&#x27;utf8&#x27;</span>)</span><br><span class=line>req = request.Request(url=url, data=data, headers=headers, method=<span class=string>&#x27;POST&#x27;</span>)</span><br><span class=line>response = request.urlopen(req)</span><br><span class=line><span class=built_in>print</span>(response.read().decode(<span class=string>&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure><p>添加请求头的第二种方式</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>from</span> urllib <span class=keyword>import</span> request, parse</span><br><span class=line></span><br><span class=line>url = <span class=string>&#x27;http://httpbin.org/post&#x27;</span></span><br><span class=line><span class=built_in>dict</span> = &#123;</span><br><span class=line>    <span class=string>&#x27;name&#x27;</span>: <span class=string>&#x27;Germey&#x27;</span></span><br><span class=line>&#125;</span><br><span class=line>data = <span class=built_in>bytes</span>(parse.urlencode(<span class=built_in>dict</span>), encoding=<span class=string>&#x27;utf8&#x27;</span>)</span><br><span class=line>req = request.Request(url=url, data=data, method=<span class=string>&#x27;POST&#x27;</span>)</span><br><span class=line>req.add_header(<span class=string>&#x27;User-Agent&#x27;</span>, <span class=string>&#x27;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&#x27;</span>)</span><br><span class=line>response = request.urlopen(req)</span><br><span class=line><span class=built_in>print</span>(response.read().decode(<span class=string>&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure><p>这种添加方式有个好处是自己可以定义一个请求头字典，然后循环进行添加</p><h4 id=高级用法各种handler>高级用法各种handler</h4><h5 id=代理-ProxyHandler>代理,ProxyHandler</h5><p>通过<code>urllib.request.ProxyHandler()</code>可以设置代理，网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问，所以这个时候需要通过设置代理来爬取数据</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>import</span> urllib.request</span><br><span class=line></span><br><span class=line>proxy_handler = urllib.request.ProxyHandler(&#123;</span><br><span class=line>    <span class=string>&#x27;http&#x27;</span>: <span class=string>&#x27;http://127.0.0.1:9743&#x27;</span>,</span><br><span class=line>    <span class=string>&#x27;https&#x27;</span>: <span class=string>&#x27;https://127.0.0.1:9743&#x27;</span></span><br><span class=line>&#125;)</span><br><span class=line>opener = urllib.request.build_opener(proxy_handler)</span><br><span class=line>response = opener.<span class=built_in>open</span>(<span class=string>&#x27;http://httpbin.org/get&#x27;</span>)</span><br><span class=line><span class=built_in>print</span>(response.read())</span><br></pre></td></tr></table></figure><h5 id=cookie-HTTPCookiProcessor>cookie,HTTPCookiProcessor</h5><p>cookie中保存中我们常见的登录信息，有时候爬取网站需要携带cookie信息访问,这里用到了<code>http.cookiejar</code>，用于获取cookie以及存储cookie</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>import</span> http.cookiejar, urllib.request</span><br><span class=line>cookie = http.cookiejar.CookieJar()</span><br><span class=line>handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class=line>opener = urllib.request.build_opener(handler)</span><br><span class=line>response = opener.<span class=built_in>open</span>(<span class=string>&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class=line><span class=keyword>for</span> item <span class=keyword>in</span> cookie:</span><br><span class=line>    <span class=built_in>print</span>(item.name+<span class=string>&quot;=&quot;</span>+item.value)</span><br></pre></td></tr></table></figure><p>同时cookie可以写入到文件中保存，有两种方式<code>http.cookiejar.MozillaCookieJar</code>和<code>http.cookiejar.LWPCookieJar()</code>，当然你自己用哪种方式都可以</p><p>具体代码例子如下：</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br></pre></td><td class=code><pre><span class=line><span class=comment># http.cookiejar.MozillaCookieJar()</span></span><br><span class=line><span class=keyword>import</span> http.cookiejar, urllib.request</span><br><span class=line>filename = <span class=string>&quot;cookie.txt&quot;</span></span><br><span class=line>cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class=line>handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class=line>opener = urllib.request.build_opener(handler)</span><br><span class=line>response = opener.<span class=built_in>open</span>(<span class=string>&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class=line>cookie.save(ignore_discard=<span class=literal>True</span>, ignore_expires=<span class=literal>True</span>)</span><br><span class=line></span><br><span class=line><span class=comment># http.cookiejar.LWPCookieJar()</span></span><br><span class=line><span class=keyword>import</span> http.cookiejar, urllib.request</span><br><span class=line>filename = <span class=string>&#x27;cookie.txt&#x27;</span></span><br><span class=line>cookie = http.cookiejar.LWPCookieJar(filename)</span><br><span class=line>handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class=line>opener = urllib.request.build_opener(handler)</span><br><span class=line>response = opener.<span class=built_in>open</span>(<span class=string>&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class=line>cookie.save(ignore_discard=<span class=literal>True</span>, ignore_expires=<span class=literal>True</span>)</span><br></pre></td></tr></table></figure><p>同样的如果想要通过获取文件中的cookie获取的话可以通过load方式，当然用哪种方式写入的，就用哪种方式读取。</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre></td><td class=code><pre><span class=line></span><br><span class=line><span class=keyword>import</span> http.cookiejar, urllib.request</span><br><span class=line>cookie = http.cookiejar.LWPCookieJar()</span><br><span class=line>cookie.load(<span class=string>&#x27;cookie.txt&#x27;</span>, ignore_discard=<span class=literal>True</span>, ignore_expires=<span class=literal>True</span>)</span><br><span class=line>handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class=line>opener = urllib.request.build_opener(handler)</span><br><span class=line>response = opener.<span class=built_in>open</span>(<span class=string>&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class=line><span class=built_in>print</span>(response.read().decode(<span class=string>&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure><h3 id=异常处理>异常处理</h3><p>在很多时候我们通过程序访问页面的时候，有的页面可能会出现错误，类似404，500等错误</p><p>这个时候就需要我们捕捉异常，下面先写一个简单的例子</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>from</span> urllib <span class=keyword>import</span> request,error</span><br><span class=line></span><br><span class=line><span class=keyword>try</span>:</span><br><span class=line>    response = request.urlopen(<span class=string>&quot;http://pythonsite.com/1111.html&quot;</span>)</span><br><span class=line><span class=keyword>except</span> error.URLError <span class=keyword>as</span> e:</span><br><span class=line>    <span class=built_in>print</span>(e.reason)</span><br></pre></td></tr></table></figure><p>上述代码访问的是一个不存在的页面，通过捕捉异常，我们可以打印异常错误</p><p>这里我们需要知道的是在urllb异常这里有两个异常错误：</p><p><code>URLError</code>，<code>HTTPError</code> —— <code>HTTPError</code>是<code>URLError</code>的子类</p><p><code>URLError</code>里只有一个属性：<code>reason</code>，即抓异常的时候只能打印错误信息，类似上面的例子</p><p><code>HTTPError</code>里有三个属性：<code>code</code>,<code>reason</code>,<code>headers</code>，即抓异常的时候可以获得<code>code</code>，<code>reason</code>，<code>headers</code>三个信息，例子如下：</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>from</span> urllib <span class=keyword>import</span> request,error</span><br><span class=line><span class=keyword>try</span>:</span><br><span class=line>    response = request.urlopen(<span class=string>&quot;http://pythonsite.com/1111.html&quot;</span>)</span><br><span class=line><span class=keyword>except</span> error.HTTPError <span class=keyword>as</span> e:</span><br><span class=line>    <span class=built_in>print</span>(e.reason)</span><br><span class=line>    <span class=built_in>print</span>(e.code)</span><br><span class=line>    <span class=built_in>print</span>(e.headers)</span><br><span class=line><span class=keyword>except</span> error.URLError <span class=keyword>as</span> e:</span><br><span class=line>    <span class=built_in>print</span>(e.reason)</span><br><span class=line></span><br><span class=line><span class=keyword>else</span>:</span><br><span class=line>    <span class=built_in>print</span>(<span class=string>&quot;reqeust successfully&quot;</span>)</span><br></pre></td></tr></table></figure><p>同时，e.reason其实也可以在做深入的判断，例子如下：</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>import</span> socket</span><br><span class=line></span><br><span class=line><span class=keyword>from</span> urllib <span class=keyword>import</span> error,request</span><br><span class=line></span><br><span class=line><span class=keyword>try</span>:</span><br><span class=line>    response = request.urlopen(<span class=string>&quot;http://www.pythonsite.com/&quot;</span>,timeout=<span class=number>0.001</span>)</span><br><span class=line><span class=keyword>except</span> error.URLError <span class=keyword>as</span> e:</span><br><span class=line>    <span class=built_in>print</span>(<span class=built_in>type</span>(e.reason))</span><br><span class=line>    <span class=keyword>if</span> <span class=built_in>isinstance</span>(e.reason,socket.timeout):</span><br><span class=line>        <span class=built_in>print</span>(<span class=string>&quot;time out&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id=URL解析>URL解析</h3><h4 id=urlparse>urlparse</h4><p>URL解析函数的重点是将URL字符串拆分为其组件，或者将URL组件组合为URL字符串。</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre></td><td class=code><pre><span class=line>urllib.parse.urlparse(urlstring, scheme=<span class=string>&#x27;&#x27;</span>, allow_fragments=<span class=literal>True</span>)</span><br></pre></td></tr></table></figure><p>功能一：</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>from</span> urllib.parse <span class=keyword>import</span> urlparse</span><br><span class=line></span><br><span class=line>result = urlparse(<span class=string>&quot;http://www.baidu.com/index.html;user?id=5#comment&quot;</span>)</span><br><span class=line><span class=built_in>print</span>(result)</span><br></pre></td></tr></table></figure><p>输出：</p><p><img onerror=imgOnError(this) data-fancybox=gallery src=/images/18bf4f402d6c09219fc6e574a3f195f8.png alt="" data-caption="" loading=lazy></p><p>这里就是可以对你传入的url地址进行拆分</p><p>同时我们是可以指定协议类型：</p><p><code>result = urlparse(&quot;www.baidu.com/index.html;user?id=5#comment&quot;,scheme=&quot;https&quot;)</code></p><p>这样拆分的时候协议类型部分就会是你指定的部分，当然如果你的url里面已经带了协议，你再通过scheme指定的协议就不会生效</p><h4 id=urlunpars>urlunpars</h4><p>其实功能和urlparse的功能相反，它是用于拼接，例子如下：</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>from</span> urllib.parse <span class=keyword>import</span> urlunparse</span><br><span class=line></span><br><span class=line>data = [<span class=string>&#x27;http&#x27;</span>,<span class=string>&#x27;www.baidu.com&#x27;</span>,<span class=string>&#x27;index.html&#x27;</span>,<span class=string>&#x27;user&#x27;</span>,<span class=string>&#x27;a=123&#x27;</span>,<span class=string>&#x27;commit&#x27;</span>]</span><br><span class=line><span class=built_in>print</span>(urlunparse(data))</span><br></pre></td></tr></table></figure><p>输出：</p><p><img onerror=imgOnError(this) data-fancybox=gallery src=/images/71eb3fb011c290339e9ba1f1db60ea77.png alt="" data-caption="" loading=lazy></p><h4 id=urljoin>urljoin</h4><p>这个的功能其实是做拼接的，例子如下：</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>from</span> urllib.parse <span class=keyword>import</span> urljoin</span><br><span class=line></span><br><span class=line><span class=built_in>print</span>(urljoin(<span class=string>&#x27;http://www.baidu.com&#x27;</span>, <span class=string>&#x27;FAQ.html&#x27;</span>))</span><br><span class=line><span class=built_in>print</span>(urljoin(<span class=string>&#x27;http://www.baidu.com&#x27;</span>, <span class=string>&#x27;https://pythonsite.com/FAQ.html&#x27;</span>))</span><br><span class=line><span class=built_in>print</span>(urljoin(<span class=string>&#x27;http://www.baidu.com/about.html&#x27;</span>, <span class=string>&#x27;https://pythonsite.com/FAQ.html&#x27;</span>))</span><br><span class=line><span class=built_in>print</span>(urljoin(<span class=string>&#x27;http://www.baidu.com/about.html&#x27;</span>, <span class=string>&#x27;https://pythonsite.com/FAQ.html?question=2&#x27;</span>))</span><br><span class=line><span class=built_in>print</span>(urljoin(<span class=string>&#x27;http://www.baidu.com?wd=abc&#x27;</span>, <span class=string>&#x27;https://pythonsite.com/index.php&#x27;</span>))</span><br><span class=line><span class=built_in>print</span>(urljoin(<span class=string>&#x27;http://www.baidu.com&#x27;</span>, <span class=string>&#x27;?category=2#comment&#x27;</span>))</span><br><span class=line><span class=built_in>print</span>(urljoin(<span class=string>&#x27;www.baidu.com&#x27;</span>, <span class=string>&#x27;?category=2#comment&#x27;</span>))</span><br><span class=line><span class=built_in>print</span>(urljoin(<span class=string>&#x27;www.baidu.com#comment&#x27;</span>, <span class=string>&#x27;?category=2&#x27;</span>))</span><br></pre></td></tr></table></figure><p>输出：</p><p><img onerror=imgOnError(this) data-fancybox=gallery src=/images/2ea662510c6b6b6119dcffa5038ed44a.png alt="" data-caption="" loading=lazy></p><p>从拼接的结果我们可以看出，拼接的时候后面的优先级高于前面的url</p><h4 id=urlencode>urlencode</h4><p>这个方法可以将字典转换为url参数，例子如下</p><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre></td><td class=code><pre><span class=line><span class=keyword>from</span> urllib.parse <span class=keyword>import</span> urlencode</span><br><span class=line></span><br><span class=line>params = &#123;</span><br><span class=line>    <span class=string>&quot;name&quot;</span>:<span class=string>&quot;zhaofan&quot;</span>,</span><br><span class=line>    <span class=string>&quot;age&quot;</span>:<span class=number>23</span>,</span><br><span class=line>&#125;</span><br><span class=line>base_url = <span class=string>&quot;http://www.baidu.com?&quot;</span></span><br><span class=line></span><br><span class=line>url = base_url+urlencode(params)</span><br><span class=line><span class=built_in>print</span>(url)</span><br></pre></td></tr></table></figure><p>输出：</p><p><img onerror=imgOnError(this) data-fancybox=gallery src=/images/53d7c7f3daf4e17112e1806425372266.png alt="" data-caption="" loading=lazy></p><h2 id=最后>最后</h2><p>这里只是简单介绍基本操作，高级操作请look官方文档。</p><p><strong>官方文档</strong> - <a target=_blank rel=noopener href=https://docs.python.org/3/library/urllib.html>https://docs.python.org/3/library/urllib.html</a></p></article><div class=nexmoe-post-copyright><strong>本文作者：</strong>Shiux<br><strong>本文链接：</strong><a href=https://blog.shiux.com/article/python-spider-3/ title=https:&#x2F;&#x2F;blog.shiux.com&#x2F;article&#x2F;python-spider-3&#x2F; target=_blank rel=noopener>https:&#x2F;&#x2F;blog.shiux.com&#x2F;article&#x2F;python-spider-3&#x2F;</a><br><strong>版权声明：</strong>本文采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh target=_blank>CC BY-NC-SA 3.0 CN</a> 协议进行许可</div><div class="nexmoe-post-meta nexmoe-rainbow"><a class="nexmoefont icon-tag-fill -none-link" href=/tags/Spider/ rel=tag>Spider</a></div><script async src="/js/copy-codeblock.js?v=1729123719849"></script><div class=nexmoe-post-footer><script src=https://giscus.app/client.js data-repo=fungyua/blog-comments data-repo-id="MDEwOlJlcG9zaXRvcnk0MDQ0NTY2OTQ=" data-category=Comments data-category-id=DIC_kwDOGBuE9s4CTUfb data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div></div></div><div class=nexmoe-post-right><div class=nexmoe-fixed><div class=nexmoe-tool><button class="mdui-fab catalog" style=overflow:unset><i class="nexmoefont icon-i-catalog"></i><div class=nexmoe-toc><ol class=toc><li class="toc-item toc-level-2"><a class=toc-link href=#%E4%BB%80%E4%B9%88%E6%98%AFUrllib><span class=toc-number>1.</span> <span class=toc-text>什么是Urllib</span></a></li><li class="toc-item toc-level-2"><a class=toc-link href=#Urllib%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8><span class=toc-number>2.</span> <span class=toc-text>Urllib基本使用</span></a><ol class=toc-child><li class="toc-item toc-level-3"><a class=toc-link href=#urlopen><span class=toc-number>2.1.</span> <span class=toc-text>urlopen</span></a><ol class=toc-child><li class="toc-item toc-level-4"><a class=toc-link href=#url%E5%8F%82%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8><span class=toc-number>2.1.1.</span> <span class=toc-text>url参数的使用</span></a></li><li class="toc-item toc-level-4"><a class=toc-link href=#data%E5%8F%82%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8><span class=toc-number>2.1.2.</span> <span class=toc-text>data参数的使用</span></a></li><li class="toc-item toc-level-4"><a class=toc-link href=#timeout%E5%8F%82%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8><span class=toc-number>2.1.3.</span> <span class=toc-text>timeout参数的使用</span></a></li><li class="toc-item toc-level-4"><a class=toc-link href=#%E5%93%8D%E5%BA%94><span class=toc-number>2.1.4.</span> <span class=toc-text>响应</span></a><ol class=toc-child><li class="toc-item toc-level-5"><a class=toc-link href=#%E5%93%8D%E5%BA%94%E7%B1%BB%E5%9E%8B%E3%80%81%E7%8A%B6%E6%80%81%E7%A0%81%E3%80%81%E5%93%8D%E5%BA%94%E5%A4%B4><span class=toc-number>2.1.4.1.</span> <span class=toc-text>响应类型、状态码、响应头</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class=toc-link href=#request><span class=toc-number>2.2.</span> <span class=toc-text>request</span></a><ol class=toc-child><li class="toc-item toc-level-4"><a class=toc-link href=#%E8%AE%BE%E7%BD%AEHeaders><span class=toc-number>2.2.1.</span> <span class=toc-text>设置Headers</span></a></li><li class="toc-item toc-level-4"><a class=toc-link href=#%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95%E5%90%84%E7%A7%8Dhandler><span class=toc-number>2.2.2.</span> <span class=toc-text>高级用法各种handler</span></a><ol class=toc-child><li class="toc-item toc-level-5"><a class=toc-link href=#%E4%BB%A3%E7%90%86-ProxyHandler><span class=toc-number>2.2.2.1.</span> <span class=toc-text>代理,ProxyHandler</span></a></li><li class="toc-item toc-level-5"><a class=toc-link href=#cookie-HTTPCookiProcessor><span class=toc-number>2.2.2.2.</span> <span class=toc-text>cookie,HTTPCookiProcessor</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class=toc-link href=#%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86><span class=toc-number>2.3.</span> <span class=toc-text>异常处理</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#URL%E8%A7%A3%E6%9E%90><span class=toc-number>2.4.</span> <span class=toc-text>URL解析</span></a><ol class=toc-child><li class="toc-item toc-level-4"><a class=toc-link href=#urlparse><span class=toc-number>2.4.1.</span> <span class=toc-text>urlparse</span></a></li><li class="toc-item toc-level-4"><a class=toc-link href=#urlunpars><span class=toc-number>2.4.2.</span> <span class=toc-text>urlunpars</span></a></li><li class="toc-item toc-level-4"><a class=toc-link href=#urljoin><span class=toc-number>2.4.3.</span> <span class=toc-text>urljoin</span></a></li><li class="toc-item toc-level-4"><a class=toc-link href=#urlencode><span class=toc-number>2.4.4.</span> <span class=toc-text>urlencode</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class=toc-link href=#%E6%9C%80%E5%90%8E><span class=toc-number>3.</span> <span class=toc-text>最后</span></a></li></ol></div></button> <a href=#nexmoe-content class="backtop toc-link" aria-label="Back To Top" title=top><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a></div></div></div></div><div id=nexmoe-footer><!--!--></div><div id=nexmoe-search-space><div class=search-container><div class=search-header><div class=search-input-container><input class=search-input type=text placeholder=搜索 oninput=sinput()></div><a class=search-close onclick=sclose()>×</a></div><div class=search-body></div></div></div><div><script async src="https://www.googletagmanager.com/gtag/js?id=G-9MEQVE108T"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-9MEQVE108T")</script></div></body></html>